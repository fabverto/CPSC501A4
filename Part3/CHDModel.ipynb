{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CHDModel.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1gCrXSWJMnX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCTvjLqFJYRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import functools\n",
        "import sys\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow_core.python.keras import regularizers\n",
        "from tensorflow.keras import layers\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ4WVuN1U_VP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_file = \"heart_train.csv\"\n",
        "test_file = \"heart_test.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2BDJMu6J42R",
        "colab_type": "code",
        "outputId": "ffa3e0ab-05e1-4914-ac4e-d24028da2138",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "!head {train_file}"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "row.names,sbp,tobacco,ldl,adiposity,famhist,typea,obesity,alcohol,age,chd\r\n",
            "56,152,5.99,7.99,32.48,Absent,45,26.57,100.32,48,0\r\n",
            "57,118,0,2.99,16.17,Absent,49,23.83,3.22,28,0\r\n",
            "58,126,5.1,2.96,26.5,Absent,55,25.52,12.34,38,1\r\n",
            "59,103,0.03,4.21,18.96,Absent,48,22.94,2.62,18,0\r\n",
            "60,121,0.8,5.29,18.95,Present,47,22.51,0,61,0\r\n",
            "61,142,0.28,1.8,21.03,Absent,57,23.65,2.93,33,0\r\n",
            "62,138,1.15,5.09,27.87,Present,61,25.65,2.34,44,0\r\n",
            "63,152,10.1,4.71,24.65,Present,65,26.21,24.53,57,0\r\n",
            "64,140,0.45,4.3,24.33,Absent,41,27.23,10.08,38,0\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01KRV62YJ5bu",
        "colab_type": "code",
        "outputId": "6c31b09a-f1d7-4e35-a7d7-5d8cf3f4f018",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "!head {test_file}"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "row.names,sbp,tobacco,ldl,adiposity,famhist,typea,obesity,alcohol,age,chd\r\n",
            "1,150,13.8,5.1,29.45,Present,52,27.92,77.76,55,1\r\n",
            "2,176,6,3.98,17.2,Present,52,21.07,4.11,61,1\r\n",
            "3,142,2.2,3.29,22.7,Absent,44,23.66,5.66,42,1\r\n",
            "4,132,0,3.3,21.61,Absent,42,24.92,32.61,33,0\r\n",
            "5,142,1.32,7.63,29.98,Present,57,31.16,72.93,33,0\r\n",
            "6,146,1.16,2.28,34.53,Absent,50,28.71,45,49,0\r\n",
            "7,132,7.2,3.65,17.16,Present,56,23.25,0,34,0\r\n",
            "8,120,0,3.57,23.22,Absent,58,27.2,0,32,0\r\n",
            "9,118,0,3.89,15.96,Absent,65,20.18,0,16,0\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUhMX9n6Z6ig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_batch(dataset):\n",
        "  for batch, label in dataset.take(1):\n",
        "    for key, value in batch.items():\n",
        "      print(\"{:20s}: {}\".format(key,value.numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zFwz2hibTn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PackNumericFeatures(object):\n",
        "  def __init__(self,names):\n",
        "    self.names = names\n",
        "  \n",
        "  def __call__(self, features, labels):\n",
        "    numeric_features = [features.pop(name) for name in self.names]\n",
        "    numeric_features = [tf.cast(feat, tf.float32) for feat in numeric_features]\n",
        "    numeric_features = tf.stack(numeric_features, axis=-1)\n",
        "    features['numeric'] = numeric_features\n",
        "\n",
        "\n",
        "    return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aC_awOYIbkj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dataset(file, **kwargs):\n",
        "  dataset = tf.data.experimental.make_csv_dataset(\n",
        "      file,\n",
        "      batch_size = 50,\n",
        "      label_name=LABEL_COLUMN,\n",
        "      na_value =\"?\",\n",
        "      num_epochs=1,\n",
        "      ignore_errors=True,\n",
        "      **kwargs)\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A82rug65KeSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL_COLUMN = 'chd'\n",
        "LABELS = [0, 1]\n",
        "\n",
        "CATEGORIES = {'famhist': ['Present', 'Absent']}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGiKzLPOKfuV",
        "colab_type": "code",
        "outputId": "49f85bed-3940-4b31-a51b-ef3385ffddc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "SELECT_COLUMNS = ['sbp','tobacco','ldl','adiposity','famhist', 'typea','obesity','alcohol','age','chd']\n",
        "raw_train_data = get_dataset(train_data, select_columns=SELECT_COLUMNS)\n",
        "raw_test_data = get_dataset(test_data, select_columns=SELECT_COLUMNS)"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-235-32ffe5448233>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mSELECT_COLUMNS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'sbp'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tobacco'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ldl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'adiposity'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'famhist'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'typea'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'obesity'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'alcohol'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'chd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mraw_train_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselect_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSELECT_COLUMNS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mraw_test_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselect_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSELECT_COLUMNS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-218-80ed43fc5abe>\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(file, **kwargs)\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mignore_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       **kwargs)\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/data/experimental/ops/readers.py\u001b[0m in \u001b[0;36mmake_csv_dataset_v2\u001b[0;34m(file_pattern, batch_size, column_names, column_defaults, label_name, select_columns, field_delim, use_quote_delim, na_value, header, num_epochs, shuffle, shuffle_buffer_size, shuffle_seed, prefetch_buffer_size, num_parallel_reads, sloppy, num_rows_for_inference, compression_type, ignore_errors)\u001b[0m\n\u001b[1;32m    424\u001b[0m   \"\"\"\n\u001b[1;32m    425\u001b[0m   \u001b[0;31m# Create dataset of all matching filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m   \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_file_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m   \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/data/experimental/ops/readers.py\u001b[0m in \u001b[0;36m_get_file_names\u001b[0;34m(file_pattern, shuffle)\u001b[0m\n\u001b[1;32m    933\u001b[0m       \u001b[0mfile_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m     \u001b[0mfile_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mget_matching_files\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mfilesystem\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0mlisting\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m   \"\"\"\n\u001b[0;32m--> 363\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mget_matching_files_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mget_matching_files_v2\u001b[0;34m(pattern)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;31m# Convert the filenames to string from bytes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatching_filename\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=g-complex-comprehension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msingle_filename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         for matching_filename in pywrap_tensorflow.GetMatchingFiles(\n\u001b[1;32m    392\u001b[0m             compat.as_bytes(single_filename))\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msingle_filename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         for matching_filename in pywrap_tensorflow.GetMatchingFiles(\n\u001b[0;32m--> 392\u001b[0;31m             compat.as_bytes(single_filename))\n\u001b[0m\u001b[1;32m    393\u001b[0m     ]\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/util/compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[0;32m---> 71\u001b[0;31m                     (bytes_or_text,))\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Expected binary or unicode string, got (OrderedDict([('famhist', <tf.Tensor: id=115244, shape=(50,), dtype=string, numpy=\narray([b'Absent', b'Present', b'Present', b'Absent', b'Absent', b'Absent',\n       b'Present', b'Absent', b'Absent', b'Absent', b'Present',\n       b'Present', b'Present', b'Present', b'Absent', b'Absent',\n       b'Present', b'Absent', b'Absent', b'Absent', b'Absent', b'Present',\n       b'Absent', b'Absent', b'Absent', b'Present', b'Absent', b'Present',\n       b'Absent', b'Absent', b'Absent', b'Present', b'Present',\n       b'Present', b'Present', b'Absent', b'Present', b'Absent',\n       b'Absent', b'Present', b'Absent', b'Present', b'Present',\n       b'Present', b'Present', b'Absent', b'Absent', b'Present',\n       b'Absent', b'Absent'], dtype=object)>), ('numeric', <tf.Tensor: id=115245, shape=(50, 8), dtype=float32, numpy=\narray([[1.1400e+02, 0.0000e+00, 8.0100e+00, 2.1640e+01, 6.6000e+01,\n        2.5510e+01, 2.4900e+00, 1.6000e+01],\n       [1.2800e+02, 0.0000e+00, 2.5100e+00, 2.9350e+01, 5.3000e+01,\n        2.2050e+01, 1.3700e+00, 6.2000e+01],\n       [1.3600e+02, 3.9900e+00, 2.5800e+00, 1.6380e+01, 5.3000e+01,\n        2.2410e+01, 2.7670e+01, 3.6000e+01],\n       [1.1600e+02, 1.0300e+00, 2.8300e+00, 1.0850e+01, 4.5000e+01,\n        2.1590e+01, 1.7500e+00, 2.1000e+01],\n       [1.0300e+02, 3.0000e-02, 4.2100e+00, 1.8960e+01, 4.8000e+01,\n        2.2940e+01, 2.6200e+00, 1.8000e+01],\n       [1.3000e+02, 0.0000e+00, 4.8900e+00, 2.5980e+01, 7.2000e+01,\n        3.0420e+01, 1.4710e+01, 2.3000e+01],\n       [1.3200e+02, 0.0000e+00, 3.5500e+00, 8.6600e+00, 6.1000e+01,\n        1.8500e+01, 3.8700e+00, 1.6000e+01],\n       [1.1200e+02, 1.4400e+00, 2.7100e+00, 2.2920e+01, 5.9000e+01,\n        2.4810e+01, 0.0000e+00, 5.2000e+01],\n       [1.1200e+02, 6.0000e-01, 5.2800e+00, 2.5710e+01, 5.5000e+01,\n        2.7020e+01, 2.7770e+01, 3.8000e+01],\n       [1.2200e+02, 0.0000e+00, 3.0500e+00, 2.3510e+01, 4.6000e+01,\n        2.5810e+01, 0.0000e+00, 3.8000e+01],\n       [1.4600e+02, 4.3600e+00, 4.3100e+00, 1.8440e+01, 4.7000e+01,\n        2.4720e+01, 1.0800e+01, 3.8000e+01],\n       [1.4000e+02, 8.0000e+00, 4.4200e+00, 3.3150e+01, 4.7000e+01,\n        3.2770e+01, 6.6860e+01, 4.4000e+01],\n       [1.3200e+02, 2.0000e+00, 2.7000e+00, 2.1570e+01, 5.0000e+01,\n        2.7950e+01, 9.2600e+00, 3.7000e+01],\n       [1.6000e+02, 1.5200e+00, 8.1200e+00, 2.9300e+01, 5.4000e+01,\n        2.5870e+01, 1.2860e+01, 4.3000e+01],\n       [1.2800e+02, 2.0000e+00, 6.1300e+00, 2.1310e+01, 6.6000e+01,\n        2.2860e+01, 1.1830e+01, 6.0000e+01],\n       [1.5800e+02, 3.6000e+00, 2.9700e+00, 3.0110e+01, 6.3000e+01,\n        2.6640e+01, 1.0800e+02, 6.4000e+01],\n       [1.7600e+02, 1.2000e+00, 8.2800e+00, 3.6160e+01, 4.2000e+01,\n        2.7810e+01, 1.1600e+01, 5.8000e+01],\n       [1.4000e+02, 5.2000e+00, 3.5800e+00, 2.9260e+01, 7.0000e+01,\n        2.7290e+01, 2.0170e+01, 4.5000e+01],\n       [1.3400e+02, 2.0000e-02, 2.8000e+00, 1.8840e+01, 4.5000e+01,\n        2.4820e+01, 0.0000e+00, 1.7000e+01],\n       [1.6200e+02, 2.9200e+00, 3.6300e+00, 3.1330e+01, 6.2000e+01,\n        3.1590e+01, 1.8510e+01, 4.2000e+01],\n       [1.2600e+02, 0.0000e+00, 4.5500e+00, 2.9180e+01, 4.8000e+01,\n        2.4940e+01, 3.6000e+01, 4.1000e+01],\n       [1.1600e+02, 2.3800e+00, 5.6700e+00, 2.9010e+01, 5.4000e+01,\n        2.7260e+01, 1.5770e+01, 5.1000e+01],\n       [1.3800e+02, 6.0000e-02, 4.1500e+00, 2.0660e+01, 4.9000e+01,\n        2.2590e+01, 2.4900e+00, 1.6000e+01],\n       [1.4200e+02, 0.0000e+00, 3.5400e+00, 1.6640e+01, 5.8000e+01,\n        2.5970e+01, 8.3600e+00, 2.7000e+01],\n       [1.5800e+02, 6.1700e+00, 8.1200e+00, 3.0750e+01, 4.6000e+01,\n        2.7840e+01, 9.2620e+01, 4.8000e+01],\n       [1.3200e+02, 2.8000e+00, 4.7900e+00, 2.0470e+01, 5.0000e+01,\n        2.2150e+01, 1.1730e+01, 4.8000e+01],\n       [1.3400e+02, 1.5000e+00, 3.7300e+00, 2.1530e+01, 4.1000e+01,\n        2.4700e+01, 1.1110e+01, 3.0000e+01],\n       [1.3800e+02, 8.8000e+00, 3.1200e+00, 2.2410e+01, 6.3000e+01,\n        2.3330e+01, 1.2003e+02, 5.5000e+01],\n       [1.5000e+02, 0.0000e+00, 4.9900e+00, 2.7730e+01, 5.7000e+01,\n        3.0920e+01, 8.3300e+00, 2.4000e+01],\n       [1.2700e+02, 0.0000e+00, 2.8100e+00, 1.5700e+01, 4.2000e+01,\n        2.2030e+01, 1.0300e+00, 1.7000e+01],\n       [1.4200e+02, 3.7200e+00, 4.2400e+00, 3.2570e+01, 5.2000e+01,\n        2.4980e+01, 7.6100e+00, 5.1000e+01],\n       [1.6200e+02, 7.0000e+00, 7.6700e+00, 3.4340e+01, 3.3000e+01,\n        3.0770e+01, 0.0000e+00, 6.2000e+01],\n       [1.1800e+02, 1.2500e+00, 4.6900e+00, 3.1580e+01, 5.2000e+01,\n        2.7160e+01, 4.1100e+00, 5.3000e+01],\n       [1.7000e+02, 7.6000e+00, 5.5000e+00, 3.7830e+01, 4.2000e+01,\n        3.7410e+01, 6.1700e+00, 5.4000e+01],\n       [1.3200e+02, 7.2000e+00, 3.6500e+00, 1.7160e+01, 5.6000e+01,\n        2.3250e+01, 0.0000e+00, 3.4000e+01],\n       [1.1800e+02, 5.4000e+00, 1.1610e+01, 3.0790e+01, 6.4000e+01,\n        2.7350e+01, 2.3970e+01, 4.0000e+01],\n       [1.6200e+02, 0.0000e+00, 5.0900e+00, 2.4600e+01, 6.4000e+01,\n        2.6710e+01, 3.8100e+00, 1.8000e+01],\n       [1.2000e+02, 0.0000e+00, 3.6800e+00, 1.2240e+01, 5.1000e+01,\n        2.0520e+01, 5.1000e-01, 2.0000e+01],\n       [1.2800e+02, 2.2400e+00, 2.8300e+00, 2.6480e+01, 4.8000e+01,\n        2.3960e+01, 4.7420e+01, 2.7000e+01],\n       [1.2800e+02, 0.0000e+00, 2.4300e+00, 1.3150e+01, 6.3000e+01,\n        2.0750e+01, 0.0000e+00, 1.7000e+01],\n       [1.4600e+02, 6.4000e-01, 4.8200e+00, 2.8020e+01, 6.0000e+01,\n        2.8110e+01, 8.2300e+00, 3.9000e+01],\n       [1.1000e+02, 2.3500e+00, 3.3600e+00, 2.6720e+01, 5.4000e+01,\n        2.6080e+01, 1.0980e+02, 5.8000e+01],\n       [1.3600e+02, 7.3600e+00, 2.1900e+00, 2.8110e+01, 6.1000e+01,\n        2.5000e+01, 6.1710e+01, 5.4000e+01],\n       [1.6200e+02, 1.5000e+00, 2.4600e+00, 1.9390e+01, 4.9000e+01,\n        2.4320e+01, 0.0000e+00, 5.9000e+01],\n       [1.7600e+02, 0.0000e+00, 3.1400e+00, 3.1040e+01, 4.5000e+01,\n        3.0180e+01, 4.6300e+00, 4.5000e+01],\n       [1.4800e+02, 0.0000e+00, 3.8400e+00, 1.7260e+01, 7.0000e+01,\n        2.0000e+01, 0.0000e+00, 2.1000e+01],\n       [1.1800e+02, 1.5000e+00, 5.3800e+00, 2.5840e+01, 6.4000e+01,\n        2.8630e+01, 3.8900e+00, 2.9000e+01],\n       [1.3400e+02, 4.8000e+00, 6.5800e+00, 2.9890e+01, 5.5000e+01,\n        2.4730e+01, 2.3660e+01, 6.3000e+01],\n       [1.3000e+02, 1.7500e+00, 5.4600e+00, 3.4340e+01, 5.3000e+01,\n        2.9420e+01, 0.0000e+00, 5.8000e+01],\n       [1.4200e+02, 2.2000e+00, 3.2900e+00, 2.2700e+01, 4.4000e+01,\n        2.3660e+01, 5.6600e+00, 4.2000e+01]], dtype=float32)>)]), <tf.Tensor: id=115246, shape=(50,), dtype=int32, numpy=\narray([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n       0, 0, 0, 0, 1, 1], dtype=int32)>)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qole6DDLdTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batch, label_batch = next(iter(raw_train_data))\n",
        "test_batch, label_batch = next(iter(raw_test_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzKaCc2ULlGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUMERIC_FEATURES = ['sbp','tobacco','ldl','adiposity', 'typea','obesity','alcohol','age']\n",
        "\n",
        "packed_train_data = raw_train_data.map(\n",
        "    PackNumericFeatures(NUMERIC_FEATURES))\n",
        "\n",
        "packed_test_data = raw_test_data.map(\n",
        "    PackNumericFeatures(NUMERIC_FEATURES))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMMGtsigLtGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batch, label_batch = next(iter(packed_train_data))\n",
        "test_batch, label_batch = next(iter(packed_test_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riXRIWkALx2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "desc = pd.read_csv(sys.argv[1])[NUMERIC_FEATURES].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmFVmsqmL6FU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MEAN = np.array(desc.T['mean'])\n",
        "STD = np.array(desc.T['std'])\n",
        "def normalize_numeric_data(data, mean, std):\n",
        "  return (data-mean)/std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIG6dDspMHan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalizer = functools.partial(normalize_numeric_data, mean=MEAN, std=STD)\n",
        "\n",
        "numeric_column = tf.feature_column.numeric_column('numeric', normalizer_fn=normalizer, shape=[len(NUMERIC_FEATURES)])\n",
        "numeric_columns = [numeric_column]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8b1KSi7MMBW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        },
        "outputId": "bf26c554-a0ff-4086-817d-6aafb82d9384"
      },
      "source": [
        "train_batch['numeric']"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=106569, shape=(50, 8), dtype=float32, numpy=\n",
              "array([[218.  ,  11.2 ,   2.77,  30.79,  38.  ,  24.86,  90.93,  48.  ],\n",
              "       [150.  ,  20.  ,   6.4 ,  35.04,  53.  ,  28.88,   8.33,  63.  ],\n",
              "       [130.  ,   0.  ,   4.89,  25.98,  72.  ,  30.42,  14.71,  23.  ],\n",
              "       [130.  ,   1.22,   3.3 ,  13.65,  50.  ,  21.4 ,   3.81,  31.  ],\n",
              "       [136.  ,   5.8 ,   5.9 ,  27.55,  65.  ,  25.71,  14.4 ,  59.  ],\n",
              "       [162.  ,   5.6 ,   4.24,  22.53,  29.  ,  22.91,   5.66,  60.  ],\n",
              "       [126.  ,   0.9 ,   5.64,  17.78,  55.  ,  21.94,   0.  ,  41.  ],\n",
              "       [148.  ,  15.  ,   4.98,  36.94,  72.  ,  31.83,  66.27,  41.  ],\n",
              "       [126.  ,   3.4 ,   4.87,  15.16,  65.  ,  22.01,  11.11,  38.  ],\n",
              "       [118.  ,   1.25,   4.69,  31.58,  52.  ,  27.16,   4.11,  53.  ],\n",
              "       [108.  ,   1.5 ,   4.33,  24.99,  66.  ,  22.29,  21.6 ,  61.  ],\n",
              "       [128.  ,   0.  ,  10.58,  31.81,  46.  ,  28.41,  14.66,  48.  ],\n",
              "       [128.  ,   6.6 ,   3.58,  20.71,  55.  ,  24.15,   0.  ,  52.  ],\n",
              "       [134.  ,   2.75,   5.51,  26.17,  57.  ,  29.87,   8.33,  33.  ],\n",
              "       [108.  ,   0.  ,   2.74,  11.17,  53.  ,  22.61,   0.95,  20.  ],\n",
              "       [134.  ,   0.  ,   5.63,  29.12,  68.  ,  32.33,   2.02,  34.  ],\n",
              "       [120.  ,   0.  ,   2.42,  16.66,  46.  ,  20.16,   0.  ,  17.  ],\n",
              "       [118.  ,   1.  ,   5.76,  22.1 ,  62.  ,  23.48,   7.71,  42.  ],\n",
              "       [116.  ,   2.38,   5.67,  29.01,  54.  ,  27.26,  15.77,  51.  ],\n",
              "       [144.  ,   0.4 ,   4.64,  30.09,  30.  ,  27.39,   0.74,  55.  ],\n",
              "       [138.  ,   0.  ,   3.14,  12.  ,  54.  ,  20.28,   0.  ,  16.  ],\n",
              "       [126.  ,   0.54,   4.39,  21.13,  45.  ,  25.99,   0.  ,  25.  ],\n",
              "       [122.  ,   0.  ,   3.08,  16.3 ,  43.  ,  22.13,   0.  ,  16.  ],\n",
              "       [124.  ,   0.61,   2.69,  17.15,  61.  ,  22.76,  11.55,  20.  ],\n",
              "       [120.  ,   0.  ,   3.57,  23.22,  58.  ,  27.2 ,   0.  ,  32.  ],\n",
              "       [118.  ,   1.62,   9.01,  21.7 ,  59.  ,  25.89,  21.19,  40.  ],\n",
              "       [138.  ,   2.  ,   5.11,  31.4 ,  49.  ,  27.25,   2.06,  64.  ],\n",
              "       [126.  ,   0.  ,   5.29,  27.64,  25.  ,  27.62,   2.06,  45.  ],\n",
              "       [190.  ,   5.15,   6.03,  36.59,  42.  ,  30.31,  72.  ,  50.  ],\n",
              "       [148.  ,   0.  ,   3.84,  17.26,  70.  ,  20.  ,   0.  ,  21.  ],\n",
              "       [126.  ,   8.75,   6.06,  32.72,  33.  ,  27.  ,  62.43,  55.  ],\n",
              "       [146.  ,   0.  ,   4.92,  18.53,  57.  ,  24.2 ,  34.97,  26.  ],\n",
              "       [174.  ,   9.45,   5.13,  35.54,  55.  ,  30.71,  59.79,  53.  ],\n",
              "       [118.  ,   4.46,   7.27,  29.13,  48.  ,  29.01,  11.11,  33.  ],\n",
              "       [128.  ,   0.73,   3.97,  23.52,  54.  ,  23.81,  19.2 ,  64.  ],\n",
              "       [136.  ,   8.8 ,   4.26,  32.03,  52.  ,  31.44,  34.35,  60.  ],\n",
              "       [126.  ,   0.  ,   4.55,  29.18,  48.  ,  24.94,  36.  ,  41.  ],\n",
              "       [136.  ,   6.8 ,   7.84,  30.74,  58.  ,  26.2 ,  23.66,  45.  ],\n",
              "       [162.  ,   2.92,   3.63,  31.33,  62.  ,  31.59,  18.51,  42.  ],\n",
              "       [142.  ,   0.28,   1.8 ,  21.03,  57.  ,  23.65,   2.93,  33.  ],\n",
              "       [130.  ,   0.  ,   1.82,  10.45,  57.  ,  22.07,   2.06,  17.  ],\n",
              "       [146.  ,   6.4 ,   5.62,  33.05,  57.  ,  31.03,   0.74,  46.  ],\n",
              "       [124.  ,   0.4 ,   3.67,  25.76,  43.  ,  28.08,  20.57,  34.  ],\n",
              "       [158.  ,   6.17,   8.12,  30.75,  46.  ,  27.84,  92.62,  48.  ],\n",
              "       [110.  ,  12.16,   4.99,  28.56,  44.  ,  27.14,  21.6 ,  55.  ],\n",
              "       [108.  ,   0.4 ,   5.91,  22.92,  57.  ,  25.72,  72.  ,  39.  ],\n",
              "       [142.  ,   0.  ,   3.54,  16.64,  58.  ,  25.97,   8.36,  27.  ],\n",
              "       [140.  ,   8.6 ,   3.9 ,  32.16,  52.  ,  28.51,  11.11,  64.  ],\n",
              "       [128.  ,   5.4 ,   2.36,  12.98,  51.  ,  18.36,   6.69,  61.  ],\n",
              "       [136.  ,   6.6 ,   6.08,  32.74,  64.  ,  33.28,   2.72,  49.  ]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 237
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7_Sg_x0MODw",
        "colab_type": "code",
        "outputId": "f747fca5-8950-4925-df87-cddeca8c732a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "numeric_layer = tf.keras.layers.DenseFeatures(numeric_columns)\n",
        "numeric_layer(train_batch).numpy()"
      ],
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=115258, shape=(50, 8), dtype=float32, numpy=\n",
              "array([[ 3.8101578 ,  1.6719555 , -0.98096985,  0.695524  , -1.4854817 ,\n",
              "        -0.26176134,  2.9621265 ,  0.36308125],\n",
              "       [ 0.5341964 ,  3.5866783 ,  0.8635502 ,  1.238294  ,  0.03767331,\n",
              "         0.7085111 , -0.3580435 ,  1.3798752 ],\n",
              "       [-0.42932174, -0.7649642 ,  0.09627039,  0.08123584,  1.9670031 ,\n",
              "         1.080208  , -0.10159453, -1.3315753 ],\n",
              "       [-0.42932174, -0.49951404, -0.7116598 , -1.4934359 , -0.2669577 ,\n",
              "        -1.096872  , -0.53972834, -0.7892852 ],\n",
              "       [-0.1402663 ,  0.4970121 ,  0.6094841 ,  0.28174144,  1.2561973 ,\n",
              "        -0.05660452, -0.11405523,  1.1087302 ],\n",
              "       [ 1.1123072 ,  0.45349562, -0.2340156 , -0.35936558, -2.3993747 ,\n",
              "        -0.7324163 , -0.46536618,  1.1765164 ],\n",
              "       [-0.6220253 , -0.5691403 ,  0.47736955, -0.96599096,  0.24076065,\n",
              "        -0.96653664, -0.6928742 , -0.11142258],\n",
              "       [ 0.43784454,  2.4987676 ,  0.14200236,  1.4809439 ,  1.9670031 ,\n",
              "         1.4205275 ,  1.9708987 , -0.11142258],\n",
              "       [-0.6220253 , -0.02518499,  0.08610775, -1.3005928 ,  1.2561973 ,\n",
              "        -0.9496414 , -0.24629928, -0.31478137],\n",
              "       [-1.0074326 , -0.49298656, -0.00535596,  0.79641527, -0.06387036,\n",
              "         0.29336968, -0.5276696 ,  0.70201254],\n",
              "       [-1.4891917 , -0.43859103, -0.18828362, -0.04519763,  1.357741  ,\n",
              "        -0.88206005,  0.17535429,  1.2443026 ],\n",
              "       [-0.5256735 , -0.7649642 ,  2.9875429 ,  0.8257886 , -0.67313236,\n",
              "         0.59507143, -0.10360432,  0.36308125],\n",
              "       [-0.5256735 ,  0.6710777 , -0.5693828 , -0.5917991 ,  0.24076065,\n",
              "        -0.43312818, -0.6928742 ,  0.6342263 ],\n",
              "       [-0.2366181 , -0.16661339,  0.41131255,  0.10550092,  0.44384798,\n",
              "         0.9474593 , -0.3580435 , -0.6537127 ],\n",
              "       [-1.4891917 , -0.7649642 , -0.9962138 , -1.8101581 ,  0.03767331,\n",
              "        -0.8048245 , -0.6546882 , -1.534934  ],\n",
              "       [-0.2366181 , -0.7649642 ,  0.47228837,  0.4822473 ,  1.5608283 ,\n",
              "         1.5412086 , -0.6116787 , -0.5859264 ],\n",
              "       [-0.9110808 , -0.7649642 , -1.1588161 , -1.1090269 , -0.67313236,\n",
              "        -1.3961601 , -0.6928742 , -1.7382929 ],\n",
              "       [-1.0074326 , -0.5473821 ,  0.5383456 , -0.4142812 ,  0.95156634,\n",
              "        -0.59484035, -0.38296485, -0.04363632],\n",
              "       [-1.1037843 , -0.24711876,  0.4926136 ,  0.46819904,  0.13921699,\n",
              "         0.31750593, -0.058987  ,  0.56644005],\n",
              "       [ 0.24514093, -0.67793137, -0.03076267,  0.6061265 , -2.297831  ,\n",
              "         0.3488827 , -0.6631293 ,  0.8375851 ],\n",
              "       [-0.04391449, -0.7649642 , -0.7929609 , -1.7041583 ,  0.13921699,\n",
              "        -1.3671966 , -0.6928742 , -1.8060791 ],\n",
              "       [-0.6220253 , -0.6474699 , -0.15779573, -0.5381606 , -0.7746761 ,\n",
              "         0.01097683, -0.6928742 , -1.1960028 ],\n",
              "       [-0.814729  , -0.7649642 , -0.8234489 , -1.1550028 , -0.9777634 ,\n",
              "        -0.9206783 , -0.6928742 , -1.8060791 ],\n",
              "       [-0.7183772 , -0.6322391 , -1.0216204 , -1.0464487 ,  0.8500227 ,\n",
              "        -0.7686204 , -0.22861312, -1.534934  ],\n",
              "       [-0.9110808 , -0.7649642 , -0.5744641 , -0.27124545,  0.5453917 ,\n",
              "         0.30302435, -0.6928742 , -0.72149897],\n",
              "       [-1.0074326 , -0.4124812 ,  2.1897752 , -0.46536538,  0.64693534,\n",
              "        -0.0131594 ,  0.15887403, -0.17920884],\n",
              "       [-0.04391449, -0.32979998,  0.20805961,  0.7734273 , -0.36850137,\n",
              "         0.31509224, -0.61007094,  1.4476614 ],\n",
              "       [-0.6220253 , -0.7649642 ,  0.29952332,  0.29323542, -2.8055494 ,\n",
              "         0.40439618, -0.61007094,  0.15972246],\n",
              "       [ 2.4612324 ,  0.35558373,  0.6755413 ,  1.4362454 , -1.0793071 ,\n",
              "         1.053658  ,  2.2012205 ,  0.49865377],\n",
              "       [ 0.43784454, -0.7649642 , -0.43726844, -1.0324005 ,  1.7639158 ,\n",
              "        -1.4347779 , -0.6928742 , -1.4671478 ],\n",
              "       [-0.6220253 ,  1.1388793 ,  0.6907851 ,  0.9420055 , -1.9932001 ,\n",
              "         0.2547519 ,  1.8165472 ,  0.8375851 ],\n",
              "       [ 0.34149274, -0.7649642 ,  0.11151446, -0.87020797,  0.44384798,\n",
              "        -0.42105982,  0.71277165, -1.1282165 ],\n",
              "       [ 1.690418  ,  1.2911868 ,  0.21822225,  1.3021494 ,  0.24076065,\n",
              "         1.1502025 ,  1.7104304 ,  0.70201254],\n",
              "       [-1.0074326 ,  0.20545204,  1.3056251 ,  0.48352417, -0.47004503,\n",
              "         0.73988837, -0.24629928, -0.6537127 ],\n",
              "       [-0.5256735 , -0.6061293 , -0.37121117, -0.23293212,  0.13921699,\n",
              "        -0.5151911 ,  0.07888447,  1.4476614 ],\n",
              "       [-0.1402663 ,  1.1497585 , -0.22385272,  0.8538849 , -0.06387036,\n",
              "         1.3263966 ,  0.6878502 ,  1.1765164 ],\n",
              "       [-0.6220253 , -0.7649642 , -0.0764944 ,  0.48990986, -0.47004503,\n",
              "        -0.24245244,  0.7541733 , -0.11142258],\n",
              "       [-0.1402663 ,  0.71459424,  1.5952606 ,  0.68913835,  0.5453917 ,\n",
              "         0.06166296,  0.25815755,  0.15972246],\n",
              "       [ 1.1123072 , -0.12962441, -0.54397607,  0.7644876 ,  0.95156634,\n",
              "         1.3626007 ,  0.05114938, -0.04363632],\n",
              "       [ 0.14878912, -0.70404124, -1.4738581 , -0.5509315 ,  0.44384798,\n",
              "        -0.55380887, -0.5751006 , -0.6537127 ],\n",
              "       [-0.42932174, -0.7649642 , -1.4636954 , -1.9021097 ,  0.44384798,\n",
              "        -0.93515986, -0.61007094, -1.7382929 ],\n",
              "       [ 0.34149274,  0.6275614 ,  0.46720693,  0.98414975,  0.44384798,\n",
              "         1.2274384 , -0.6631293 ,  0.22750872],\n",
              "       [-0.7183772 , -0.67793137, -0.5236508 ,  0.05313959, -0.9777634 ,\n",
              "         0.51542217,  0.13395263, -0.5859264 ],\n",
              "       [ 0.9196036 ,  0.5775175 ,  1.7375375 ,  0.69041544, -0.67313236,\n",
              "         0.4574955 ,  3.0300574 ,  0.36308125],\n",
              "       [-1.3928398 ,  1.8808345 ,  0.14708357,  0.41072917, -0.87621975,\n",
              "         0.28854233,  0.17535429,  0.8375851 ],\n",
              "       [-1.4891917 , -0.67793137,  0.61456525, -0.30955854,  0.44384798,\n",
              "        -0.05419086,  2.2012205 , -0.2469951 ],\n",
              "       [ 0.14878912, -0.7649642 , -0.5897081 , -1.1115812 ,  0.5453917 ,\n",
              "         0.00614949, -0.35683763, -1.0604303 ],\n",
              "       [ 0.05243731,  1.1062421 , -0.4067804 ,  0.8704874 , -0.06387036,\n",
              "         0.6192077 , -0.24629928,  1.4476614 ],\n",
              "       [-0.5256735 ,  0.40997925, -1.1893041 , -1.579002  , -0.16541404,\n",
              "        -1.8306104 , -0.42396453,  1.2443026 ],\n",
              "       [-0.1402663 ,  0.6710777 ,  0.70094776,  0.94455975,  1.1546537 ,\n",
              "         1.7705011 , -0.5835417 ,  0.43086752]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 238
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BXNmG65MXZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categorical_columns = []\n",
        "for feature, vocab in CATEGORIES.items():\n",
        "  cat_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "        key=feature, vocabulary_list=vocab)\n",
        "  categorical_columns.append(tf.feature_column.indicator_column(cat_col))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pALQEo_jMfuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categorical_layer = tf.keras.layers.DenseFeatures(categorical_columns)\n",
        "preprocessing_layer = tf.keras.layers.DenseFeatures(categorical_columns+numeric_columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FC9IMOiMnQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  preprocessing_layer,\n",
        "  layers.Dense(256, activation='relu'),\n",
        "  layers.Dropout(0.7),\n",
        "   layers.Dense(128, activation='relu'),\n",
        "  layers.Dropout(0.5),\n",
        "   layers.Dense(128, activation='selu'),\n",
        "  layers.Dropout(0.3),\n",
        "  layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkG6eoWaMqeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = packed_train_data.shuffle(500)\n",
        "test_data = packed_test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6H_AExPMsa5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "66efc0be-9f63-4162-a68c-58c016f51d89"
      },
      "source": [
        "\n",
        "print(\"--Train--\")\n",
        "model.fit(train_data, epochs=200)\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_data)\n",
        "\n",
        "print('\\n\\nTest Loss {}, Test Accuracy {}'.format(test_loss, test_accuracy * 100))"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--Train--\n",
            "Epoch 1/200\n",
            "9/9 [==============================] - 1s 80ms/step - loss: 0.7184 - accuracy: 0.5848\n",
            "Epoch 2/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6227 - accuracy: 0.6757\n",
            "Epoch 3/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6065 - accuracy: 0.6658\n",
            "Epoch 4/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.6150 - accuracy: 0.6757\n",
            "Epoch 5/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.6311 - accuracy: 0.6757\n",
            "Epoch 6/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5659 - accuracy: 0.7199\n",
            "Epoch 7/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5511 - accuracy: 0.7248\n",
            "Epoch 8/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5949 - accuracy: 0.7052\n",
            "Epoch 9/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5595 - accuracy: 0.7297\n",
            "Epoch 10/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5180 - accuracy: 0.7420\n",
            "Epoch 11/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5734 - accuracy: 0.7027\n",
            "Epoch 12/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5246 - accuracy: 0.7273\n",
            "Epoch 13/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5541 - accuracy: 0.7224\n",
            "Epoch 14/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5608 - accuracy: 0.7125\n",
            "Epoch 15/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5333 - accuracy: 0.7125\n",
            "Epoch 16/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5175 - accuracy: 0.7494\n",
            "Epoch 17/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5302 - accuracy: 0.7445\n",
            "Epoch 18/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5353 - accuracy: 0.7199\n",
            "Epoch 19/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5233 - accuracy: 0.7297\n",
            "Epoch 20/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5570 - accuracy: 0.7248\n",
            "Epoch 21/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5184 - accuracy: 0.7273\n",
            "Epoch 22/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5554 - accuracy: 0.7543\n",
            "Epoch 23/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5595 - accuracy: 0.7420\n",
            "Epoch 24/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5367 - accuracy: 0.7396\n",
            "Epoch 25/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5174 - accuracy: 0.7396\n",
            "Epoch 26/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5292 - accuracy: 0.7396\n",
            "Epoch 27/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5087 - accuracy: 0.7592\n",
            "Epoch 28/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5371 - accuracy: 0.7543\n",
            "Epoch 29/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4913 - accuracy: 0.7617\n",
            "Epoch 30/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5091 - accuracy: 0.7494\n",
            "Epoch 31/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5298 - accuracy: 0.7396\n",
            "Epoch 32/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4818 - accuracy: 0.7592\n",
            "Epoch 33/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4927 - accuracy: 0.7617\n",
            "Epoch 34/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5370 - accuracy: 0.7543\n",
            "Epoch 35/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5185 - accuracy: 0.7371\n",
            "Epoch 36/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5189 - accuracy: 0.7297\n",
            "Epoch 37/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5007 - accuracy: 0.7371\n",
            "Epoch 38/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5064 - accuracy: 0.7666\n",
            "Epoch 39/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4920 - accuracy: 0.7592\n",
            "Epoch 40/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.4936 - accuracy: 0.7592\n",
            "Epoch 41/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5370 - accuracy: 0.7568\n",
            "Epoch 42/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4930 - accuracy: 0.7494\n",
            "Epoch 43/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.4676 - accuracy: 0.7445\n",
            "Epoch 44/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5159 - accuracy: 0.7715\n",
            "Epoch 45/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5263 - accuracy: 0.7518\n",
            "Epoch 46/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5508 - accuracy: 0.7641\n",
            "Epoch 47/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4923 - accuracy: 0.7518\n",
            "Epoch 48/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5477 - accuracy: 0.7518\n",
            "Epoch 49/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4966 - accuracy: 0.7764\n",
            "Epoch 50/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4766 - accuracy: 0.7789\n",
            "Epoch 51/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4964 - accuracy: 0.7764\n",
            "Epoch 52/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4843 - accuracy: 0.7494\n",
            "Epoch 53/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4618 - accuracy: 0.7641\n",
            "Epoch 54/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5414 - accuracy: 0.7666\n",
            "Epoch 55/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4499 - accuracy: 0.7641\n",
            "Epoch 56/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4256 - accuracy: 0.7813\n",
            "Epoch 57/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4778 - accuracy: 0.7764\n",
            "Epoch 58/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4493 - accuracy: 0.7862\n",
            "Epoch 59/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5281 - accuracy: 0.7715\n",
            "Epoch 60/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4841 - accuracy: 0.7617\n",
            "Epoch 61/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.4790 - accuracy: 0.7518\n",
            "Epoch 62/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4869 - accuracy: 0.7592\n",
            "Epoch 63/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4844 - accuracy: 0.7690\n",
            "Epoch 64/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4918 - accuracy: 0.7469\n",
            "Epoch 65/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4466 - accuracy: 0.7641\n",
            "Epoch 66/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5210 - accuracy: 0.7641\n",
            "Epoch 67/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4700 - accuracy: 0.7543\n",
            "Epoch 68/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4893 - accuracy: 0.7568\n",
            "Epoch 69/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5276 - accuracy: 0.7592\n",
            "Epoch 70/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4742 - accuracy: 0.7617\n",
            "Epoch 71/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4594 - accuracy: 0.7740\n",
            "Epoch 72/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4743 - accuracy: 0.7862\n",
            "Epoch 73/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4693 - accuracy: 0.7690\n",
            "Epoch 74/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4860 - accuracy: 0.7543\n",
            "Epoch 75/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4889 - accuracy: 0.7518\n",
            "Epoch 76/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4567 - accuracy: 0.7666\n",
            "Epoch 77/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4696 - accuracy: 0.7813\n",
            "Epoch 78/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5179 - accuracy: 0.7666\n",
            "Epoch 79/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4633 - accuracy: 0.7813\n",
            "Epoch 80/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4639 - accuracy: 0.7838\n",
            "Epoch 81/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4950 - accuracy: 0.7715\n",
            "Epoch 82/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4757 - accuracy: 0.7666\n",
            "Epoch 83/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4899 - accuracy: 0.7568\n",
            "Epoch 84/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4869 - accuracy: 0.7592\n",
            "Epoch 85/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4636 - accuracy: 0.7813\n",
            "Epoch 86/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4732 - accuracy: 0.7740\n",
            "Epoch 87/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4760 - accuracy: 0.7764\n",
            "Epoch 88/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4591 - accuracy: 0.7813\n",
            "Epoch 89/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4540 - accuracy: 0.7961\n",
            "Epoch 90/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5318 - accuracy: 0.7641\n",
            "Epoch 91/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4666 - accuracy: 0.7666\n",
            "Epoch 92/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4598 - accuracy: 0.7887\n",
            "Epoch 93/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4682 - accuracy: 0.7862\n",
            "Epoch 94/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4401 - accuracy: 0.7936\n",
            "Epoch 95/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4299 - accuracy: 0.7838\n",
            "Epoch 96/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4292 - accuracy: 0.7912\n",
            "Epoch 97/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.4597 - accuracy: 0.7715\n",
            "Epoch 98/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.4873 - accuracy: 0.7641\n",
            "Epoch 99/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4380 - accuracy: 0.7887\n",
            "Epoch 100/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4594 - accuracy: 0.7740\n",
            "Epoch 101/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4732 - accuracy: 0.7666\n",
            "Epoch 102/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4490 - accuracy: 0.7568\n",
            "Epoch 103/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4633 - accuracy: 0.8010\n",
            "Epoch 104/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4916 - accuracy: 0.7666\n",
            "Epoch 105/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4699 - accuracy: 0.7912\n",
            "Epoch 106/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4482 - accuracy: 0.8010\n",
            "Epoch 107/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4459 - accuracy: 0.7617\n",
            "Epoch 108/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4264 - accuracy: 0.8010\n",
            "Epoch 109/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4856 - accuracy: 0.7764\n",
            "Epoch 110/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4501 - accuracy: 0.7715\n",
            "Epoch 111/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5100 - accuracy: 0.7789\n",
            "Epoch 112/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4410 - accuracy: 0.7936\n",
            "Epoch 113/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.4569 - accuracy: 0.7985\n",
            "Epoch 114/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4746 - accuracy: 0.7641\n",
            "Epoch 115/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4827 - accuracy: 0.7862\n",
            "Epoch 116/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4074 - accuracy: 0.7912\n",
            "Epoch 117/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4445 - accuracy: 0.7789\n",
            "Epoch 118/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4257 - accuracy: 0.7912\n",
            "Epoch 119/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4339 - accuracy: 0.7912\n",
            "Epoch 120/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4444 - accuracy: 0.7838\n",
            "Epoch 121/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4411 - accuracy: 0.7838\n",
            "Epoch 122/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4714 - accuracy: 0.7715\n",
            "Epoch 123/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4535 - accuracy: 0.7912\n",
            "Epoch 124/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4672 - accuracy: 0.7740\n",
            "Epoch 125/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4182 - accuracy: 0.8084\n",
            "Epoch 126/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4659 - accuracy: 0.7936\n",
            "Epoch 127/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4732 - accuracy: 0.7764\n",
            "Epoch 128/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.5073 - accuracy: 0.7764\n",
            "Epoch 129/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4515 - accuracy: 0.7617\n",
            "Epoch 130/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4679 - accuracy: 0.7887\n",
            "Epoch 131/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4206 - accuracy: 0.7862\n",
            "Epoch 132/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4383 - accuracy: 0.8108\n",
            "Epoch 133/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4483 - accuracy: 0.7813\n",
            "Epoch 134/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4479 - accuracy: 0.7838\n",
            "Epoch 135/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4288 - accuracy: 0.7764\n",
            "Epoch 136/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.3982 - accuracy: 0.8010\n",
            "Epoch 137/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4208 - accuracy: 0.7912\n",
            "Epoch 138/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4191 - accuracy: 0.8182\n",
            "Epoch 139/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.5194 - accuracy: 0.7813\n",
            "Epoch 140/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4699 - accuracy: 0.7985\n",
            "Epoch 141/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.4446 - accuracy: 0.7838\n",
            "Epoch 142/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4228 - accuracy: 0.8059\n",
            "Epoch 143/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4738 - accuracy: 0.7936\n",
            "Epoch 144/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4277 - accuracy: 0.7912\n",
            "Epoch 145/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4481 - accuracy: 0.7912\n",
            "Epoch 146/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4396 - accuracy: 0.8010\n",
            "Epoch 147/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.3983 - accuracy: 0.8108\n",
            "Epoch 148/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4473 - accuracy: 0.7912\n",
            "Epoch 149/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5254 - accuracy: 0.7936\n",
            "Epoch 150/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4222 - accuracy: 0.8010\n",
            "Epoch 151/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4298 - accuracy: 0.7838\n",
            "Epoch 152/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4225 - accuracy: 0.7789\n",
            "Epoch 153/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.3911 - accuracy: 0.7985\n",
            "Epoch 154/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.3972 - accuracy: 0.7936\n",
            "Epoch 155/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4755 - accuracy: 0.7936\n",
            "Epoch 156/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4156 - accuracy: 0.7912\n",
            "Epoch 157/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4093 - accuracy: 0.7936\n",
            "Epoch 158/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.4331 - accuracy: 0.7961\n",
            "Epoch 159/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4476 - accuracy: 0.7936\n",
            "Epoch 160/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4072 - accuracy: 0.7887\n",
            "Epoch 161/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4126 - accuracy: 0.8059\n",
            "Epoch 162/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4127 - accuracy: 0.7936\n",
            "Epoch 163/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4359 - accuracy: 0.7936\n",
            "Epoch 164/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4265 - accuracy: 0.7887\n",
            "Epoch 165/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.3969 - accuracy: 0.7961\n",
            "Epoch 166/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4389 - accuracy: 0.7985\n",
            "Epoch 167/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4072 - accuracy: 0.7936\n",
            "Epoch 168/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4196 - accuracy: 0.8059\n",
            "Epoch 169/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.4132 - accuracy: 0.8059\n",
            "Epoch 170/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4129 - accuracy: 0.8010\n",
            "Epoch 171/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4703 - accuracy: 0.7813\n",
            "Epoch 172/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4050 - accuracy: 0.7985\n",
            "Epoch 173/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4667 - accuracy: 0.8059\n",
            "Epoch 174/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.3910 - accuracy: 0.8206\n",
            "Epoch 175/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.3992 - accuracy: 0.8034\n",
            "Epoch 176/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4164 - accuracy: 0.7936\n",
            "Epoch 177/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4141 - accuracy: 0.7912\n",
            "Epoch 178/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4477 - accuracy: 0.7936\n",
            "Epoch 179/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4357 - accuracy: 0.8034\n",
            "Epoch 180/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.3823 - accuracy: 0.8108\n",
            "Epoch 181/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4014 - accuracy: 0.8305\n",
            "Epoch 182/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4335 - accuracy: 0.8010\n",
            "Epoch 183/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4537 - accuracy: 0.7985\n",
            "Epoch 184/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4306 - accuracy: 0.7936\n",
            "Epoch 185/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.4225 - accuracy: 0.8010\n",
            "Epoch 186/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.3904 - accuracy: 0.8182\n",
            "Epoch 187/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4089 - accuracy: 0.7912\n",
            "Epoch 188/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.3932 - accuracy: 0.7838\n",
            "Epoch 189/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4399 - accuracy: 0.7862\n",
            "Epoch 190/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.4199 - accuracy: 0.8182\n",
            "Epoch 191/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4079 - accuracy: 0.7887\n",
            "Epoch 192/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4109 - accuracy: 0.8010\n",
            "Epoch 193/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4102 - accuracy: 0.7740\n",
            "Epoch 194/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4199 - accuracy: 0.7887\n",
            "Epoch 195/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4239 - accuracy: 0.8010\n",
            "Epoch 196/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4030 - accuracy: 0.8108\n",
            "Epoch 197/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4150 - accuracy: 0.8059\n",
            "Epoch 198/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3870 - accuracy: 0.8182\n",
            "Epoch 199/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4194 - accuracy: 0.8206\n",
            "Epoch 200/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4301 - accuracy: 0.8133\n",
            "2/2 [==============================] - 0s 76ms/step - loss: 0.2695 - accuracy: 0.8182\n",
            "\n",
            "\n",
            "Test Loss 0.2695312798023224, Test Accuracy 81.81818127632141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDEbE2CbVZns",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "outputId": "6eafba72-413d-4e4e-a6eb-8a01e548a078"
      },
      "source": [
        "# Show some results\n",
        "predictions = model.predict(test_data)\n",
        "for prediction, CHD in zip(predictions[:50], list(test_data)[0][1][:50]):\n",
        "  print(\"Predicted Coronary Heart Disease: {:.2%}\".format(prediction[0]),\n",
        "        \" | Actual outcome: \",\n",
        "        (\"YES Coronary Heart Disease\" if bool(CHD) else \"NO Coronary Heart Disease\"))"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Coronary Heart Disease: 56.28%  | Actual outcome:  YES Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 33.10%  | Actual outcome:  YES Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 15.21%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 0.11%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 30.90%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 11.58%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 75.79%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 13.22%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 21.91%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 66.76%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 25.85%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 0.40%  | Actual outcome:  YES Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 1.45%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 0.54%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 35.80%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 2.53%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 0.85%  | Actual outcome:  YES Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 10.00%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 13.88%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 33.55%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 25.48%  | Actual outcome:  YES Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 25.73%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 0.13%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 16.92%  | Actual outcome:  YES Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 1.11%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 0.46%  | Actual outcome:  YES Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 16.80%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 2.25%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 7.75%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 26.04%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 97.60%  | Actual outcome:  YES Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 35.03%  | Actual outcome:  YES Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 3.66%  | Actual outcome:  YES Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 8.67%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 90.27%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 9.69%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 36.85%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 1.13%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 23.57%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 10.65%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 22.06%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 7.63%  | Actual outcome:  YES Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 18.83%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 1.30%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 43.36%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 24.07%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 23.08%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 21.56%  | Actual outcome:  YES Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 25.69%  | Actual outcome:  NO Coronary Heart Disease\n",
            "Predicted Coronary Heart Disease: 5.00%  | Actual outcome:  YES Coronary Heart Disease\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}